{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d0eea0f",
   "metadata": {},
   "source": [
    "# Questao 1"
   ]
  },
  {
   "cell_type": "code",
   "id": "44938bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:37:02.251558Z",
     "start_time": "2025-06-15T14:35:17.308143Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "print(\"Carregando o dataset de reviews de filmes...\")\n",
    "reviews_train = load_files(\"aclImdb/train/\", categories=['pos', 'neg'], encoding='utf-8', random_state=42)\n",
    "X_text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "reviews_test = load_files(\"aclImdb/test/\", categories=['pos', 'neg'], encoding='utf-8', random_state=42)\n",
    "X_text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(f\"Número de documentos de treino: {len(X_text_train)}\")\n",
    "print(f\"Número de documentos de teste: {len(X_text_test)}\")\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(min_df=5, stop_words=\"english\", ngram_range=(1, 2)),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'tfidfvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]\n",
    "}\n",
    "\n",
    "print(\"Iniciando GridSearchCV (pode levar algum tempo)...\")\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_text_train, y_train)\n",
    "\n",
    "print(f\"\\nMelhor pontuação de validação cruzada: {grid.best_score_:.2f}\")\n",
    "print(f\"Melhores parâmetros: {grid.best_params_}\")\n",
    "\n",
    "test_score = grid.score(X_text_test, y_test)\n",
    "print(f\"Pontuação de teste final: {test_score:.2f}\")\n",
    "\n",
    "vectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "coef = grid.best_estimator_.named_steps[\"logisticregression\"].coef_.ravel()\n",
    "sorted_coef_indices = coef.argsort()\n",
    "\n",
    "print(\"\\nFeatures mais negativas (associadas a sentimentos negativos):\")\n",
    "print(feature_names[sorted_coef_indices[:20]])\n",
    "\n",
    "print(\"\\nFeatures mais positivas (associadas a sentimentos positivos):\")\n",
    "print(feature_names[sorted_coef_indices[::-1][:20]])\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset de reviews de filmes...\n",
      "Número de documentos de treino: 20476\n",
      "Número de documentos de teste: 19895\n",
      "Iniciando GridSearchCV (pode levar algum tempo)...\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "Melhor pontuação de validação cruzada: 0.89\n",
      "Melhores parâmetros: {'logisticregression__C': 10, 'tfidfvectorizer__ngram_range': (1, 3)}\n",
      "Pontuação de teste final: 0.88\n",
      "\n",
      "Features mais negativas (associadas a sentimentos negativos):\n",
      "['worst' 'awful' 'bad' 'waste' 'boring' 'worse' 'poor' 'poorly'\n",
      " 'disappointment' 'terrible' 'dull' 'horrible' 'fails' 'annoying'\n",
      " 'disappointing' 'unfortunately' 'ridiculous' 'mess' 'waste time' 'save']\n",
      "\n",
      "Features mais positivas (associadas a sentimentos positivos):\n",
      "['great' 'excellent' 'perfect' 'best' 'wonderful' 'amazing' 'favorite'\n",
      " 'today' 'superb' '10 10' 'fantastic' 'enjoyable' 'brilliant' 'loved'\n",
      " 'entertaining' 'love' 'fun' 'refreshing' 'enjoyed' 'perfectly']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "3c94bc27",
   "metadata": {},
   "source": [
    "# Questao 2\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7611407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T14:18:32.382657Z",
     "start_time": "2025-06-15T14:18:28.916781Z"
    }
   },
   "source": [
    "X_train_tfidf = vectorizer.transform(X_text_train)\n",
    "max_tfidf = X_train_tfidf.max(axis=0).toarray().ravel()\n",
    "sorted_by_tfidf = max_tfidf.argsort()[::-1]\n",
    "\n",
    "print(\"\\nTop 10 features com maiores valores de TF-IDF:\")\n",
    "for idx in sorted_by_tfidf[:10]:\n",
    "    print(f\"{feature_names[idx]}: {max_tfidf[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 features com menores valores de TF-IDF:\")\n",
    "for idx in sorted_by_tfidf[-10:]:\n",
    "    print(f\"{feature_names[idx]}: {max_tfidf[idx]:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 features com maiores valores de TF-IDF:\n",
      "pokemon: 0.8838\n",
      "doodlebops: 0.8533\n",
      "casper: 0.8468\n",
      "raj: 0.8114\n",
      "dev: 0.8113\n",
      "zizek: 0.8079\n",
      "darkman: 0.8050\n",
      "demons: 0.7955\n",
      "joan: 0.7833\n",
      "montand: 0.7782\n",
      "\n",
      "Top 10 features com menores valores de TF-IDF:\n",
      "enrico ratso rizzo: 0.0637\n",
      "enrico ratso: 0.0637\n",
      "scene stealing: 0.0627\n",
      "video game movie: 0.0625\n",
      "intercontinental: 0.0623\n",
      "vampire cloak: 0.0621\n",
      "literally br: 0.0589\n",
      "literally br br: 0.0589\n",
      "didn help br: 0.0582\n",
      "stubbornly: 0.0574\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "35628f30",
   "metadata": {},
   "source": "# Questão 3:\n"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deaa9f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Originais (primeiras 5 observações):\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Features Normalizadas (L2, primeiras 5 observações):\n",
      " [[0.80377277 0.55160877 0.22064351 0.0315205 ]\n",
      " [0.82813287 0.50702013 0.23660939 0.03380134]\n",
      " [0.80533308 0.54831188 0.2227517  0.03426949]\n",
      " [0.80003025 0.53915082 0.26087943 0.03478392]\n",
      " [0.790965   0.5694948  0.2214702  0.0316386 ]]\n",
      "\n",
      "Norma L2 da primeira observação normalizada: 12.247448713915889\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
